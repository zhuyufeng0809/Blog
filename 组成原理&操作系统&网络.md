### 高速缓存

* 为了弥补处理器与主内存处理速度之间的鸿沟，硬件设计者在主内存和处理器之间引人了**高速缓存（Cache）**，高速缓存是一种存取速率远比主内存大而容量远比主内存小的存储部件
* 引人高速缓存之后，处理器在执行内存读、写操作的时候并不直接与主内存打交道，而是通过高速缓存进行
* 变量名相当于内存地址，而变量值则相当于相应内存空间所存储的数据，高速缓存相当于为进程所访问的每个变量保留了一份相应内存空间所存储数据（变量值）的副本
* 由于高速缓存的存储容量远小于主内存，因此高速缓存并不是每时每刻保留着所有变量值的副本
* 高速缓存相当于一个由硬件实现的容量极小的散列表（Hash Table），其键（Key）是一个内存地址，其值（Value）是内存数据的副本或者准备写人内存的数据。从内部结构来看，高速缓存相当于一个拉链散列表（Chained Hash Table），它包含若干桶（Bucket，硬件上称之为Set），每个桶又可以包含若干缓存条目（Cache Entry），缓存条目可被进一步划分为Tag、DataBlock以及Flag这三个部分
    * DataBlock也被称为缓存行（CacheLine），它是高速缓存与主内存之间的数据交换最小单元，用于存储从内存中读取的或者准备写往内存的数据，一个缓存行可以存储若干变量的值，而多个变量的值则可能被存储在同一个缓存行之中
    * Tag包含了与缓存行中数据相应的内存地址的部分信息（内存地址的高位部分比特）
    * Flag用于表示相应缓存行的状态信息
* 处理器在执行内存访问操作时会将相应的内存地址解码。内存地址的解码结果包括tag、index以及offset这三部分数据
    * index相当于桶编号，它可以用来定位内存地址对应的桶，一个桶可能包含多个缓存条目
    * tag相当于缓存条目的相对编号，其作用在于用来与同一个桶中的各个缓存条目中的Tag部分进行比较，以定位一个具体的缓存条目
    * 一个缓存条目中的缓存行可以用来存储多个变量，offset是缓存行内的位置偏移，其作用在于确定一个变量在一个缓存行中的存储起始位置
* 根据内存地址的解码结果，如果在高速缓存中能够找到相应的缓存行并且缓存行所在的缓存条目的Flag表示相应缓存条目是有效的，那么就称相应的内存操作产生了缓存命中（CacheHit）；否则，就称相应的内存操作产生了缓存未命中（Cache Miss）。缓存未命中包括读未命中（Read Miss）和写未命中（Write Miss），分别对应内存读和写操作
    * 当产生读未命中时，处理器所需读取的数据会从主内存中加载并被存人相应的缓存行之中。这个过程会导致处理器停顿（Stall）而不能执行其他指令，这不利于发挥处理器的处理能力。因此，从性能的角度来看应该尽可能地减少缓存未命中
    * 由于高速缓存的总容量远小于主内存的总容量，同一个缓存行在不同时刻存储的可能是不同的一段数据，因此缓存未命中是不可避免的
* 现代处理器一般具有多个层次的高速缓存，相应层级的高速缓存通常被称为一级缓存（L1 Cache）、二级缓存（L2 Cache）、三级缓存（L3 Cache），多核CPU里的每一个CPU核，都有独立的属于自己的L1 Cache和L2 Cache。多个CPU之间，共用L3 Cache和主内存
    * 一级缓存被直接集成在处理器的内核（Core）里，每个核上都有一个L1缓存，因此其访问效率非常高，一级缓存通常包括两部分，其中一部分用于存储指令（L1i），另外一部分用于存储数据（L1d）
    * L2缓存更大一些，速度要慢一些，一般情况下每个核上都有一个独立的L2缓存
    * L3缓存是三级缓存中最大的一级，同时也是最慢的一级, 在同一个CPU插槽之间的核共享一个L3缓存
* 距离处理器越近的高速缓存，其存取速率越快，制造成本越高，因此其容量也越小。距离处理器越远（即距离主内存越近）的高速缓存，其存储速率会越慢，而存储容量则相应地增大
* 读取数据过程：就像数据库缓存一样，首先在最快的缓存中找数据，如果缓存没有命中(Cache miss)则往下一级找，直到三级缓存都找不到时，向内存要数据。一次次地未命中，代表取数据消耗的时间越长
* 计算过程：程序以及数据被加载到主内存；指令和数据被加载到CPU的高速缓存；CPU执行指令，把结果写到高速缓存；高速缓存中的数据写回主内存

### 缓存一致性协议
多核CPU的每个核有各自的一级和二级缓存，一个核对其副本数据进行更新之后，其他核如何“察觉”到该更新并做出适当反应，以确保这些核后续读取该共享变量时能够读取到这个更新，这就是缓存一致性问题，为了解决这个问题，处理器之间需要一种通信机制一缓存一致性协议（Cache Coherence Protocol）  

MESI（Modified-Exclusive-Shared-Invalid）协议是一种广为使用的缓存一致性协议，x86处理器所使用的缓存一致性协议就是基于MESI协议的。MESI协议要求对同一地址的读内存操作是并发的，而针对同一地址的写内存操作是独占的，即针对同一内存地址进行的写操作在任意一个时刻只能够由一个核执行。在MESI协议中，一个处理器往内存中写数据时必须持有该数据的所有权  

为了保障数据的一致性，MESI将缓存条目的状态划分为Modified、Exclusive、 Shared和Invalid这 4种，并在此基础上定义了一组消息（Message）用于协调各个处理器的读、写内存操作。MESI协议中一个缓存条目的Flag值有以下4种可能

| 状态 | 描述 |
| :----: | :----: |
| M（Modified）| 该状态表示相应缓存行有效。该状态表示相应缓存行包含对相应内存地址所做的更新结果数据。由于MESI协议中的任意一个时刻只能够有一个核对同一内存地址对应的数据进行更新，因此在多核处理器的高速缓存中Tag值相同的缓存条目中，任意一个时刻只能够有一个缓存条目处于该状态，且其他所有核上的高速缓存当前都不保留该数据的有效副本。缓存行中包含的数据与主内存中包含的数据不一致 |
| E（Exclusive）| 该状态表示相应缓存行有效。该缓存行以独占的方式保留了相应内存地址的副本数据，即其他所有核上的高速缓存当前都不保留该数据的有效副本。缓存行中包含的数据与主内存中包含的数据一致|
| S（Shared）| 该状态表示相应缓存行有效。其他核上的高速缓存中也**可能**存在相同内存地址对应的副本数据，其他副本缓存条目的状态也为Shared。缓存行中包含的数据与主内存中包含的数据一致|
| I（Invalid）| 该状态表示相应缓存行无效。该状态是缓存条目的初始状态 |

MESI协议定义了一组消息（Message）用于协调各个处理器的读、写内存操作。可以将MESI协议中的消息分为请求消息和响应消息。处理器核心在执行内存读、写操作时在必要的情况下会往总线（Bus）中发送特定的请求消息，同时每个处理器核心还嗅探（Snoop，也称拦截）总线中由其他处理器核心发出的请求消息并在一定条件下往总线中回复相应的响应消息

| 状态 | 消息类型 | 描述 |
| :----: | :----: | :----: |
| Read | 请求 | 通知其他处理器核心、主内存当前处理器核心准备读取某个数据。该消息包含待读取数据的内存地址 |
| Read Response | 响应 | 该消息包含被请求读取的数据。该消息可能是主内存提供的，也可能是嗅探 Read消息的其他高速缓存提供的 |
| Invalidate | 请求 | 通知其他处理器核心将其高速缓存中指定内存地址对应的缓存条目状态置为I，即通知这些处理器核心删除指定内存地址的副本数据 |
| Invalidate Acknowledge | 响应 | 接收到Invalidate消息的处理器核心必须回复该消息，以表示删除了其高速缓存上的相应副本数据 |
| Read Invalidate | 请求 | 该消息是由Read消息和Invalidate消息组合而成的复合消息。用于通知其他处理器核心当前处理器准备更新（Read-Modify-Write，读后写更新）一个数据，并请求其他处理器核心删除其高速缓存中相应的副本数据。接收到该消息的处理器核心必须回复Read Response消息和Invalidate Acknowledge消息 |
| Writeback | 请求 | 该消息包含需要写入主内存的数据及其对应的内存地址 |

#### 读过程
发起读操作的CPU核心会根据地址找到对应的缓存条目，读取该缓存条目的Flag值（缓存条目状态），如果找到的缓存条目的状态如果为M、E或者S，那么该CPU核心可以直接从相应的缓存行中读取地址所对应的数据，而无须往总线中发送任何消息。如果该CPU核心找到的缓存条目的状态如果为I，则说明该CPU核心的高速缓存中并不包含有效副本数据，此时该CPU核心需要往总线发送Read消息以读取地址对应的数据，而其他CPU核心（或者主内存）则需要回复Read Response以提供相应的数据。发起读操作的CPU核 心接收到Read Response消息时，会将其中携带的数据（包含数据的数据块）存人相应的缓存行并将相应缓存条目的状态更新为S。该CPU核心接收到的Read Response消息可能来自主内存也可能来自其他CPU核心

* 如果其他CPU核心嗅探到Read消息时，找到的相应缓存条目的状态为M，那么其他CPU核心在往总线发送Read Response消息前将相应缓存行中的数据写入主内存，再发送Read Response消息。其他CPU核心往总线发送Read Response之后，相应缓存条目的状态会被更新为S
* 如果其他CPU核心嗅探到Read消息时，找到的相应缓存条目的状态为E，那么其他CPU核心在往总线发送Read Response消息后，相应缓存条目的状态会被更新为S
* 如果其他CPU核心嗅探到Read消息时，找到的相应缓存条目的状态为I，那么发起读操作的CPU核心所接收到的Read Response消息就来自主内存

#### 写过程
任何一个CPU核心执行内存写操作时必须拥有相应数据的所有权。发起写操作的CPU核心会根据地址找到对应的缓存条目。如果发起写操作的CPU核心所找到的缓存条目的状态若为E或者M，则说明该CPU核心已经拥有相应数据的所有权，此时该CPU核心可以直接将数据写人相应的缓存行并将相应缓存条目的状态更新为M（如果本来状态就为M，则无须更新）。如果发起写操作的CPU核心所找到的缓存条目的状态如果不为E、M，则该CPU核心需要往总线发送Invalidate消息以获得数据的所有权。其他CPU核心接收到Invalidate消息后会将其高速缓存中相应的缓存条目状态更新为I（相应的副本数据置为无效）并回复Invalidate Acknowledge消息。发送Invalidate消息的CPU核心（即发起写操作的CPU核心），必须在接收到其他所有CPU核心所回复的所有Invalidate Acknowledge消息之后再将数据更新到相应的缓存行之中

* 如果发起写操作的CPU核心找到的缓存条目的状态为S，则说明其他CPU核心上的高速缓存可能也保留了地址对应的数据副本，此时发起写操作的CPU核心需要往总线发送Invalidate消息。发起写操作的CPU核心在接收到其他所有CPU核心所回复的Invalidate Acknowledge消息之后会将相应的缓存条目的状态更新为E，此时发起写操作的CPU核心获得了地址上数据的所有权。接着，发起写操作的CPU核心便可以将数据写人相应的缓存行，并将相应的缓存条目的状态更新为M
* 如果发起写操作的CPU核心找到的缓存条目的状态为I，此时该CPU核心需要往总线发送Read Invalidate消息。发起写操作的CPU核心在接收到Read Response消息以及其他所有CPU核心所回复的Invalidate Acknowledge消息之后，会将相应缓存条目的状态更新为E，这表示该CPU核心已经获得相应数据的所有权。接着，发起写操作的CPU核心便可以往相应的缓存行中写人数据了并将相应缓存条目的状态更新为M。其他CPU核心在接收到Invalidate消息或者Read Invalidate消息之后，必须根据消息中包含的内存地址在该处理器的高速缓存中查找相应的高速缓存条目。若其他CPU核心所找到的高速缓存条目的状态不为I，那么其他CPU核心必须将相应缓存条目的状态更新为I，把相应的副本数据置为无效并给总线回复Invalidate Acknowledge消息

### 写缓冲器和无效化队列
MESI协议解决了缓存一致性问题，但是存在一个性能弱点：发起写操作的CPU核心未获得相应数据的所有权时，必须等待其他所有CPU核心将其高速缓存中的相应副本数据删除并接收到这些CPU核心所回复的Invalidate Acknowledge/Read Response消息，获得相应数据的所有权之后才能将数据写人高速缓存。为了规避和减少这种等待造成的写操作的延迟（Latency），硬件设计者引入了写缓冲器和无效化队列

#### 写缓冲器
写缓冲器（Store Buffer，也被称为Write Buffer）是处理器内部的一个容量比高速缓存还小的私有高速存储部件，每个CPU核心都有其写缓冲器，写缓冲器内部可包含若干条目（Entry）。一个CPU核心无法读取另外一个CPU核心上的写缓冲器中的内容  

引入写缓冲器之后，CPU核心在执行写操作时会做这样的处理：

* 如果相应的缓存条目状态为E或者M，CPU核心可能会直接将数据写人相应的缓存行而无须发送任何消息
* 如果相应的缓存条目状态为S，CPU核心会先将写操作的相关数据（包括数据和待操作的内存地址）存入写缓冲器的条目之中，并发送Invalidate消息
* 如果相应的缓存条目状态为I，则表明相应的写操作遇到了写未命中（Write Miss），此时CPU核心会先将写操作相关数据存人写缓冲器的条目之中，并发送Read Invalidate消息。在其他所有CPU核心的高速缓存都未保存指定地址的副本数据的情况下，Read消息回复者是主内存，也就是说Read消息可能导致主内存读操作，这种情况下的写未命中开销是比较大的

发起写操作的CPU核心在将写操作的相关数据写入写缓冲器之后便认为该写操作已经完成，即该CPU核心并不等待其他CPU核心返回Invalidate Acknowledge/Read Response消息而是继续执行其他指令（比如执行读操作）。一个CPU核心接收到其他CPU核心所回复的针对同一个缓存条目的所有Invalidate Acknowledge消息的时候，该CPU核心会将写缓冲器中针对相应地址的写操作的结果写人相应的缓存行中，此时写操作对于发起写操作的CPU核心之外的其他CPU核心来说才算是完成的

#### 无效化队列
引人无效化队列（Invalidate Queue）之后，CPU核心在接收到Invalidate消息之后并不删除消息中指定地址对应的副本数据，而是将消息存人无效化队列之后就回复InvalidateAcknowledge消息，从而减少了发起写操作的CPU核心所需的等待时间  

写缓冲器和无效化队列的引人又会带来一些新的问题：内存重排序和可见性问题
#### 存储转发
引入写缓冲器之后，CPU核心在执行读操作的时候不能根据相应的内存地址直接读取相应缓存行中的数据作为该操作的结果。这是因为一个CPU核心在更新一个变量之后紧接着又读取该变量的值的时候，由于该CPU核心先前对该变量的更新结果可能仍然还停留在写缓冲器之中，因此该变量相应的内存地址所对应的缓存行中存储的值是该变量的旧值。这种情况下为了避免读操作所返回的结果是一个旧值，CPU核心在执行读操作的时候会根据相应的内存地址查询写缓冲器。如果写缓冲器存在相应的条目，那么该条目所代表的写操作的结果数据就会直接作为该读操作的结果返回；否则，CPU核心才从高速缓存中读取数据。这种CPU核心直接从写缓冲器中读取数据来实现内存读操作的技术被称为存储转发（StoreForwarding）。存储转发使得发起写操作的CPU核心能够在不影响该处理器执行读操作的情况下将写操作的结果存入写缓冲器

#### 内存重排序
写缓冲器和无效化队列都可能导致内存重排序

##### 写缓冲器可能导致StoreLoad重排序（Stores Reordered After Loads）
StoreLoad重排序是绝大多数处理器都允许的一种内存重排序

| Processor0 | Processor1 |
| :----: | :----: |
| X=1;//S1 | Y=1;//S3 |
| r1=Y;//L2 |  |
|  | r2=X;//L4 |

假设CPU核心Processor0和Processor1上的两个线程未使用任何同步措施而各自按照程序顺序并依照上表所示的线程交错顺序执行。其中变量X、Y为共享变量，其初始值均为0，r1、r2为局部变量。当Processor0上的线程执行到L2时，虽然在此之前S3已经被Processor1执行完毕，但是由于S3的执行结果可能仍然还停留在Processor1的写缓冲器中，而一个CPU核心无法读取另外一个CPU核心的写缓冲器中的内容，因此 Processor0此刻读取到的Y的值仍然是其高速缓存中存储的该变量的初始值0。同理，Processor1执行到 L4时所读取到变量X的值也可能是该变量的初始值0。因此，从Processor1的角度来看，Processor1执行L4的那一刻Processor0已经执行了L2而S1却像是尚未被执行，即Processor1对Processor0执行的两个操作的感知顺序是L2→S1，也就是说此时写缓冲器导致了S1被重排序到了L2之后

##### 写缓冲器可能导致StoreStore重排序（Stores Reordered After Stores）

| Processor0 | Processor1 |
| :----: | :----: |
| data=1;//S1 |  |
| ready=true;//S2 |  |
|  | while(!ready) continue;//L3 |
|  | print(data);//L4 |

假设CPU核心Processor0和Processor1上的两个线程未使用任何同步措施而各自按照程序顺序并依照上表所示的线程交错顺序执行。其中变量data、ready为共享变量，其初始值分别为0和false。假设Processor0执行S1、S2时该处理器的高速缓存中包含变量ready的副本但不包含变量data的副本，那么S1的执行结果会先被存入写缓冲器而S2的执行结果会直接被存人高速缓存。L3被执行时S2对ready的更新通过缓存一致性协议可以被Processor1读取到，于是，由于ready值已变为true，因此Processor1继续执行L4。L4被执行的时候，由于S1对data的更新结果可能仍然停留在ProcessorO的写缓冲器之中，因此Processor1此时读取到的变量data的值可能仍然是其初始值0，即L4的输出结果可能仍然是0而不是Processor1所期望的新值（Processor0更新之后的值）。从Processor1的角度来看，这就造成了一种现象：S2像是先于S1被执行，即S1被重排序到了S2之后

##### 无效化队列可能导致LoadLoad重排序（Loads Reordered After Loads）

| Processor0 | Processor1 |
| :----: | :----: |
| data=1;//S1 |  |
| ready=true;//S2 |  |
|  | while(!ready) continue;//L3 |
|  | print(data);//L4 |

假设CPU核心Processor0和Processor1上的两个线程未使用任何同步措施而各自按照程序顺序并依照上表所示的线程交错顺序执行。其中变量data、ready为共享变量，其初始值分别为0和false。进一步假设Processor0的高速缓存中存有变量data和ready的副本，Processor1仅存有变量data的副本而未存有变量ready的副本。那么，Processor0和Processor1有可能按照如下序列执行一系列操作：

* Processor0执行S1。此时由于Processor1上也存有变量data的副本，因此Processor0会发出Invalidate消息并将S1的操作结果存人写缓冲器
* Processor1接收到Processor0发出的Invalidate消息时将该消息存人其无效化队列并回复Invalidate Acknowledge消息
* Processor0接收到Invalidate Acknowledge消息，随即将S1的操作结果写人高速缓存。然后，Processor0执行S2。此时由于只有Processor0上存有变量ready的副本，因此Processor0无须发送任何消息，直接将S2的操作结果存人高速缓存即可
* Processor1执行L3。此时由于Processor1的高速缓存中并没有存储变量ready的副本，因此Processor1会发出一个Read消息
* Processor0接收到Processor1发出的Read消息并回复Read Response消息。由于此时Processor0已经执行过S2，因此该ReadResponse消息包含的ready变量值为true
* Processor1接收到Read Response消息并从中取出ready变量的新值，此时L3中的循环语句可以结束
* Processor1执行L4。此时，由于Processor0为了更新变量data而发出的Invalidate消息可能仍然还停留在Processor1的无效化队列中，因此Processor1从其高速缓存中读取的变量data的值仍然是其初始值。因此，L4所打印的变量值可能是一个旧值

由此可见，尽管Processor0对共享变量data，ready的更新是按照程序顺序先后到达高速缓存的，但是由于无效化队列的作用Processor1像是在ready变量不为true的情况下提前读取了变量data的值，然而，程序的实际处理逻辑是仅在ready变量值为true的情况下才读取变量data，因此这里Processor1实际读取到的变量（data）值是一个旧值。也就是说，从Processor0的角度来看，L4被重排序到了L3之前

不同的处理器架构所支持（允许）的内存重排序各有不同。比如，现代处理器都会采用写缓冲器，而有的处理器（比如x86）会保障写操作的顺序，即这些处理器不允许StoreStore重排序的出现

#### 可见性

##### 写缓冲器导致的可见性问题
写缓冲器是CPU核心内部的私有存储部件，一个CPU核心中的写缓冲器所存储的内容是无法被其他CPU核心所读取的。因此，一个CPU核心上运行的线程更新了一个共享变量之后，其他CPU核心上运行的线程再来读取该变量时这些线程可能仍然无法读取到前一个线程对该变量所做的更新，因为这个更新可能还停留在前一个线程所在的CPU核心上的写缓冲器之中。这种现象就是所谓的可见性问题。因此，硬件根源上的可见性问题一半是由写缓冲器导致的。为了使一个CPU核心上运行的线程对共享变量所做的更新可以被其他CPU核心上运行的其他线程所读取，必须将写缓冲器中的内容写入其所在的CPU核心上的高速缓存之中，从而使该更新在缓存一致性协议的作用下可以被其他CPU核心读取到  

CPU核心在一些特定条件下（比如写缓冲器满、I/O指令被执行）会将写缓冲器排空（Drain）或者冲刷（Flush），即将写缓冲器中的内容写入高速缓存，但是从程序对一个或者一组变量更新的角度来看，CPU核心本身并不保证这种冲刷对程序来说是“及时”的。因此，为了保证一个CPU核心对共享变量所做的更新可以被其他CPU核心同步，编译器等底层系统需要借助一类被称为内存屏障的特殊指令。内存屏障中的存储屏障（Store Barrier）可以使执行该指令的CPU核心冲刷其写缓冲器

##### 无效化队列导致的可见性问题
硬件根源上可见性问题的另一半是由无效化队列导致的。无效化队列的引入本身也会导致新的问题：CPU核心在执行内存读取操作前如果没有根据无效化队列中的内容将该CPU核心上的高速缓存中的相关副本数据删除，那么就可能导致该CPU核心读到的数据是过时的旧数据，从而使得其他CPU核心所做的更新丢失。因此。为了使一个CPU核心运行的线程能够读取到另外一个CPU核心上运行的线程对共享变量所做的更新，该CPU核心必须先根据无效化队列中存储的Invalidate消息删除其高速缓存中的相应副本数据，从而使其他CPU核心上运行的线程对共享变量所做的更新在缓存一致性协议的作用下能够被同步到该CPU核心的高速缓存之中。内存屏障的加载屏障（Load Barrier）正是用来解决这个问题的。加载屏障会根据无效化队列内容所指定的内存地址，将相应CPU核心上的高速缓存中相应的缓存条目的状态都标记为I，从而使该CPU核心后续执行针对相应地址（无效化队列内容中指定的地址）的读内存操作时必须发送Read消息，以将其他CPU核心对相关共享变量所做的更新同步到该CPU核心的高速缓存中

##### 存储转发导致的可见性问题
假设CPU核心Processor0在t1时刻更新了某个共享变量，随后又在t2时刻读取了该变量。在t1时刻到t2时刻之间的这段时间内其他CPU核心可能已经更新了该共享变量，但是如果t2时刻Processor0在t1时刻所做的更新仍然停留在该CPU核心的写缓冲器之中，那么存储转发技术会使Processor0直接从其写缓冲器读取该共享变量的值。也就是说Processor0此时根本不从高速缓存中读取该变量的值，这就使得另外一个CPU核心对该共享变量所做的更新无法被该CPU核心读取，从而导致Processor0在t2时刻读取到的变量值是一个旧值。因此，考虑到存储转发技术的这个副作用，从读线程的角度来看，为了使读线程能够将其他线程对共享变量所做的更新同步到该线程所在的CPU核心的高速缓存中，需要清空该CPU核心上的写缓冲器（先）以及无效化队列（后）  

因此，解决可见性问题首先要使写线程所在的CPU核心对共享变量所做的更新能够到达（被存储到）高速缓存，从而使该更新对其他CPU核心是可同步的。其次，读线程所在的CPU核心要将其无效化队列中的内容“应用”到其高速缓存上，这样才能够将其他CPU核心对共享变量所做的更新同步到该CPU核心的高速缓存中。而这两点是通过存储屏障与加载屏障的成对使用实现的：写线程的执行CPU核心所执行的存储屏障保际了该线程对共享变量所做的更新对读线程来说是可同步的；读线程的执行CPU核心所执行的加载屏障将写线程对共享变量所做的更新同步到该CPU核心的高速缓存之中

### 基本内存屏障
处理器支持（允许）哪种内存重排序（LoadLoad重排序、LoadStore重排序、StoreStore重排序、StoreLoad重排序），就会提供能够禁止相应重排序的指令，这些指令就被称为**基本内存屏障**

* LoadLoad屏障
* LoadStore屏障
* StoreStore屏障
* StoreLoad屏障

基本内存屏障可以统一用XY来表示，其中的X和Y可以代表Store和Load。基本内存屏障是对一类指令的称呼，这类指令的作用是禁止该指令左侧的任何X操作与该指令右侧的任何Y操作之间进行重排序，从而确保该指令左侧的所有X操作先于该指令右侧的Y操作被提交，即内存操作作用到高速缓存（或者主内存）上

| 屏障名称 | 示例指令序列 | 具体作用 |
| :----: | :----: | :----: |
| StoreLoad | Store1;Store2;Store3;**StoreLoad**;Load1;Load2;Load3 | 禁止StoreLoad重排序，即确保该屏障之前的任何一个写操作（比如Store2）的结果都会在该屏障之后的任何一个读操作（比如Load1）的数据被加载之前对其他CPU核心来说是可同步的 |
| StoreStore | Store1;Store2;Store3;**StoreStore**;Store4;Store5;Store6 | 禁止StoreStore重排序，即确保该屏障之前的任何一个写操作（比如Store1）的结果都会在该屏障之后的任何一个写操作（比如Store4）之前对其他CPU核心来说是可同步的 |
| LoadLoad | Load1;Load2;Load3;**LoadLoad**;Load4;Load5;Load6 | 禁止LoadLoad重排序，即确保该屏障之前的任何一个读操作（比如Load1）的数据都会在该屏障之后的任何一个读操作（比如Load4）之前被加载 |
| LoadStore | Load1;Load2;Load3;**LoadStore**;Store1;Store2;Store3; | 禁止LoadStore重排序，即确保该屏障之前的任何一个读操作（比如Load1）的数据都会在该屏障之后的任何一个写操作（比如Store1）的结果被冲刷（写入）到高速缓存（或者主内存）之前被加载 |

基本内存屏障的作用只是保障其左侧的X操作先于其右侧的Y操作被提交，它并不全面禁止重排序。**XY屏障两侧的内存操作仍然可以在不越过内存屏障本身的情况下在各自的范围内进行重排序，并且XY屏障左侧的非X操作与屏障右侧的非Y操作之间仍然可以进行重排序（即越过内存屏障本身）**。例如，在Store1;Load1;Store2;**StoreLoad**;Store3;Load2;Load3指令序列中，Load2、Load3和Store1、Store2之间无法进行重排序，而Store1、Load1和Store2之间可以重排序，Store3、Load2和Load3之间可以重排序，Load1和Store3之间也可以进行重排序  

内存屏障需要编译器（JIT编译器）、运行时（Java虚拟机）和处理器等多方的尊重，才能保障其作用得以落实  

LoadLoad屏障是通过清空无效化队列来实现禁止LoadLoad重排序的。LoadLoad屏障会使其执行CPU核心根据无效化队列中的Invalidate消息删除其高速缓存中相应的副本。这个过程被称为将无效化队列应用到高速缓存，也被称为清空无效化队列，它使CPU核心有机会将其他CPU核心对共享变量所做的更新同步到该CPU核心的高速缓存中，从而消除了LoadLoad重排序的根源而实现了禁止LoadLoad重排序  

StoreStore屏障可以通过对写缓冲器中的条目进行标记来实现禁止StoreStore重排序。StoreStore屏障会将写缓冲器中的现有条目做一个标记，以表示这些条目代表的写操作需要先于该屏障之后的写操作被提交。CPU核心在执行写操作的时候如果发现写缓冲器中存在被标记的条目，那么即使这个写操作对应的高速缓存条目的状态为E或者M，此时处理器也不直接将写操作的数据写入高速缓存，而是将其写入写缓冲器，从而使得StoreStore屏障之前的任何写操作先于该屏障之后的写操作被提交  

就处理器的具体实现而言，许多处理器往往将StoreLoad屏障实现为一个通用基本内存屏障（General-purpose Barrier），即StoreLoad屏障能够实现其他3种基本内存屏障的效果。StoreLoad屏障能够替代其他基本内存屏障，但是它的开销也是最大的：StoreLoad屏障会清空无效化队列，并将写缓冲器中的条目冲刷（写入）高速缓存。因此，StoreLoad屏障既可以将其他CPU核心对共享变量所做的更新同步到该CPU核心的高速缓存中，又可以使其执行CPU核心对共享变量所做的更新对其他CPU核心来说可同步

https://www.cnblogs.com/valjeanshaw/p/11469514.html
https://blog.csdn.net/qyf__123/article/details/100904595

https://www.cnblogs.com/yanlong300/p/8986041.html
https://blog.csdn.net/unei66/article/details/25738977

https://blog.csdn.net/qq_35494088/article/details/79845240
https://www.jianshu.com/p/64240319ed60

https://blog.csdn.net/qq_25330791/article/details/105551921

https://www.cnblogs.com/kaleidoscope/p/9598140.html


### I/O

#### LinuxI/O机制：

##### 缓存I/O（Buffered I/O）：

缓存I/O又被称作标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，操作系统会将I/O的数据缓存在文件系统的页缓存（page cache）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。缓存I/O有以下这些优点

* 缓存I/O使用了操作系统内核缓冲区，在一定程度上分离了应用程序空间和实际的物理设备
* 缓存I/O可以减少读盘的次数，从而提高性能

对于读操作来说，当应用程序尝试读取某块数据的时候，如果这块数据已经存放在了页缓存中，那么这块数据就可以立即返回给应用程序，而不需要经过实际的物理读盘操作。当然，如果数据在应用程序读取之前并未被存放在页缓存中，那么就需要先将数据从磁盘读到页缓存中去

对于写操作来说，应用程序也会将数据先写到页缓存中去，数据是否被立即写到磁盘上去取决于应用程序所采用的写操作机制：
* 如果用户采用的是同步写机制（synchronous writes），那么数据会立即被写回到磁盘上，应用程序会一直等到数据被写完为止
* 如果用户采用的是延迟写机制（deferred writes），那么应用程序就完全不需要等到数据全部被写回到磁盘，数据只要被写到页缓存中去就可以了。在延迟写机制的情况下，操作系统会定期地将放在页缓存中的数据刷到磁盘上。**与异步写机制（asynchronous writes）不同的是，延迟写机制在数据完全写到磁盘上的时候不会通知应用程序，而异步写机制在数据完全写到磁盘上的时候是会返回给应用程序的。所以延迟写机制本身是存在数据丢失的风险的，而异步写机制则不会有这方面的担心**

缓存I/O的缺点

* 在缓存I/O机制中，DMA方式可以将数据直接从磁盘读到页缓存中，或者将数据从页缓存直接写回到磁盘上，而不能直接在应用程序地址空间和磁盘之间进行数据传输，这样的话，数据在传输过程中需要在应用程序地址空间和页缓存之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的

##### 内存映射方式I/O

在很多操作系统包括Linux中，内存区域（memory region）是可以跟一个普通的文件或者块设备文件的某一个部分关联起来的，若进程要访问内存页中某个字节的数据，操作系统就会将访问该内存区域的操作转换为相应的访问文件的某个字节的操作。Linux中提供了系统调用mmap()来实现这种文件访问方式。与标准的访问文件的方式相比，内存映射方式可以减少标准访问文件方式中read()系统调用所带来的数据拷贝操作，即减少数据在用户地址空间和操作系统内核地址空间之间的拷贝操作。映射通常适用于较大范围，对于相同长度的数据来讲，映射所带来的开销远远低于CPU拷贝所带来的开销。当大量数据需要传输的时候，采用内存映射方式去访问文件会获得比较好的效率

mmap内存映射的实现过程，总的来说可以分为三个阶段：

* 进程启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域
* 调用内核空间的系统调用mmap，实现文件物理地址和进程虚拟地址的一一映射关系
* 进程发起对这片映射空间的访问，引发缺页异常，实现磁盘文件内容到物理内存（主存）的拷贝

总结来说，常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过了两次数据拷贝过程，才能完成进程对文件内容的获取任务。写操作也是一样，待写入的buffer在内核空间不能直接访问，必须要先拷贝至内核空间对应的主存，再写回磁盘中（延迟写回），也是需要两次数据拷贝

而使用mmap操作文件中，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，只使用一次数据拷贝

总而言之，常规文件操作需要从磁盘到页缓存再到用户主存的两次数据拷贝。而mmap操控文件，只需要一次数据拷贝过程

##### 自缓存应用程序（self-caching applications）

对于某些特殊的应用程序来说，避开操作系统内核缓冲区而直接在应用程序地址空间和磁盘之间传输数据会比使用操作系统内核缓冲区获取更好的性能，自缓存应用程序就是其中的一种。对于某些应用程序来说，它会**有它自己的数据缓存机制**，比如，它会将数据缓存在应用程序地址空间，这类应用程序完全不需要使用操作系统内核中的高速缓冲存储器，这类应用程序就被称作是自缓存应用程序（self-caching applications）。数据库管理系统是这类应用程序的一个代表。自缓存应用程序对要操作的数据的语义了如指掌，所以它可以采用更加高效的缓存替换算法

Linux中的直接I/O技术非常适用于自缓存这类应用程序，该技术省略掉缓存I/O技术中操作系统内核缓冲区的使用，数据直接在应用程序地址空间和磁盘之间进行传输，从而使得自缓存应用程序可以省略掉复杂的系统级别的缓存结构，而执行程序自己定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。Linux中的直接I/O机制为自缓存应用程序提供了很好的支持

##### 直接I/O（Direct I/O）：

凡是通过直接I/O方式进行数据传输，数据均直接在用户地址空间的缓冲区和磁盘之间直接进行传输，完全不需要页缓存的支持。操作系统层提供的缓存往往会使应用程序在读写数据的时候获得更好的性能，但是对于某些特殊的应用程序，比如说数据库管理系统这类应用，他们更倾向于选择他们自己的缓存机制，因为数据库管理系统往往比操作系统更了解数据库中存放的数据，数据库管理系统可以提供一种更加有效的缓存机制来提高数据库中数据的存取性能

直接I/O最主要的优点就是通过减少操作系统内核缓冲区和应用程序地址空间的数据拷贝次数，降低了对文件读取和写入时所带来的CPU的使用以及内存带宽的占用。这对于某些特殊的应用程序，比如自缓存应用程序来说，不失为一种好的选择。如果要传输的数据量很大，使用直接I/O的方式进行数据传输，而不需要操作系统内核地址空间拷贝数据操作的参与，这将会大大提高性能

直接I/O并不一定总能提供令人满意的性能上的飞跃。设置直接I/O的开销非常大，而直接I/O又不能提供缓存I/O的优势。缓存I/O的读操作可以从高速缓冲存储器中获取数据，而直接I/O的读数据操作会造成磁盘的同步读，这会带来性能上的差异，并且导致进程需要较长的时间才能执行完；对于写数据操作来说，使用直接I/O需要write()系统调用同步执行，否则应用程序将会不知道什么时候才能够再次使用它的I/O缓冲区。与直接I/O读操作类似的是，直接I/O写操作也会导致应用程序关闭缓慢。所以，应用程序使用直接I/O进行数据传输的时候通常会和使用异步I/O结合使用

参考：
https://blog.csdn.net/liuwg1226/article/details/107345743
https://www.ibm.com/developerworks/cn/linux/l-cn-directio/
https://zhuanlan.zhihu.com/p/96391501
https://segmentfault.com/a/1190000021471509?utm_source=tag-newest
https://www.imooc.com/wenda/detail/555617
https://blog.csdn.net/zqixiao_09/article/details/51088478?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-3&spm=1001.2101.3001.4242
https://www.zhihu.com/question/48161206
首先非匿名映射的mmap只是做了映射，没有把数据加载到内存中，其次mmap只是映射到了用户空间。在后面访问的时候，如果没有加载到内存就会产生缺页异常，陷入内核，内核会在分配出对应的物理页，并把文件数据从磁盘读到物理内存中，然后把物理页与虚拟地址建立映射，这样间接映射了虚拟地址与文件，用户就可以读写操作了。

我估计你没清楚物理内存和虚拟地址的概念，所有的物理内存肯定都是要被内核管理的，而虚拟地址在x86 32位机器上只有3G至4G的虚拟地址空间是内核空间（你所说的内核空间应该是指这里），3G以下是给用户用的（什么stack,head,bss,data,text段都是用户空间的），mmap的操作映射的地址范围是3G以下的范围，因此在缺页异常后，无论内核在物理地址的哪里分配物理页，都会映射到3G以下的虚拟地址，也就是用户空间，并没有在用户空间操作内核空间的内容。

https://blog.csdn.net/u012129558/article/details/82878994
https://blog.csdn.net/gong_1/article/details/18405153

https://www.zhihu.com/question/32163005

##### 零拷贝
https://blog.csdn.net/weixin_37782390/article/details/103833306
https://www.xttblog.com/?p=5101
https://blog.csdn.net/keil_wang/article/details/86688271
https://www.jianshu.com/p/7863667d5fa7
https://www.jianshu.com/p/a4325188f974
https://mp.weixin.qq.com/s?__biz=MzUyNzgyNzAwNg==&mid=2247483929&idx=1&sn=ed536aee5ac7f898fc5e640785769fd4&chksm=fa78eb48cd0f625e8020cc062deb4bfb3739cff289ca50404242003270ca97a4a51df2f84815&cur_album_id=1638279915399593986&scene=189#rd
https://mp.weixin.qq.com/s?__biz=MzUyNzgyNzAwNg==&mid=2247483933&idx=1&sn=d9776b9efe054b30523adbe60cb7524a&chksm=fa78eb4ccd0f625aaf68b5fadcbc8dffd2f191307b311de2cf3345fb2f6f29238749af7158af&cur_album_id=1638279915399593986&scene=189#rd

#### LinuxI/O模型：

Linux的内核将所有外部设备都看做一个文件来操作，对一个文件的读写操作会调用内核提供的系统命令，返回一个file descriptor (fd, 文件描述符)。而对一个socket的读写也会有相应的描述符，称为socketfd(socket描述符)，描述符就是一个数字，它指向内核中的一个结构体(文件路径，数据区等一些属性)。UNIX提供了5种I/O模型：

##### 阻塞I/O模型

最常用的I/O模型就是阻塞I/O模型，缺省情形下，所有文件操作都是阻塞的。以套接字接口为例：在进程空间中调用recvfrom，其系统调用直到数据包到达且被复制到应用进程的缓冲区中或者发生错误时才返回，在此期间一直会等待，进程在从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此被称为阻塞I/0模型

##### 非阻塞I/O模型

recvfrom从应用层到内核的时候，如果该缓冲区没有数据的话，就直接返回一个EWOULDBLOCK错误，一般都对非阻塞I/O模型进行轮询检查这个状态，看内核是不是有数据到来

##### I/O复用模型

Linux提供select/poll，进程通过将一个或多个fd传递给select或poll系统调用，阻塞在select操作上，这样select/poll可以帮我们侦测多个fd是否处于就绪状态。select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限，因此它的使用受到了一些制约。**Linux还提供了一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback**

##### 信号驱动I/O模型

首先开启套接口信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数(此系统调用立即返回，进程继续工作，它是非阻塞的)。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理数据

##### 异步I/O

告知内核启动某个操作，并让内核在整个操作完成后(包括将数据从内核复制到用户自己的缓冲区)通知我们。这种模型与信号驱动模型的主要区别是：信号驱动I/O由内核通知我们何时可以开始一个I/O 操作;异步I/O模型由内核通知我们I/O操作何时已经完成

#### I/O多路复用

对于大多数Java程序员来说，不需要了解网络编程的底层细节，只需要有个概念，了解了网络编程的基础知识后，理解Java的NIO类库就会更加容易一些。Java NIO的核心类库多路复用器Selector是基于epoll的多路复用技术实现的

在I/O编程过程中，当需要同时处理多个客户端接入请求时，可以利用多线程或者I/O多路复用技术进行处理。**I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求**。与传统的多线程/多进程模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源，I/O多路复用的主要应用场景如下

* 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字
* 服务器需要同时处理多种网络协议的套接字

目前支持I/O多路复用的系统调用有select、pselect、poll、epoll，在Linux网络编程过程中，很长一段时间都使用select做轮询和网络事件通知，然而select的一些固有缺陷导致了它的应用受到了很大的限制，最终Linux不得不在新的内核版本中寻找select的替代方案，最终选择了epoll。epoll与select的原理比较类似，为了克服select的缺点，epoll作了很多重大改进

* 支持一个进程打开的socket描述符( FD)不受限制(仅受限于操作系统的最大文件句柄数)。select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。对于那些需要支持上万个TCP连接的大型服务器来说显然太少了。可以选择修改这个宏然后重新编译内核，不过这会带来网络效率的下降。也可以通过选择多进程的方案(传统的Apache方案)解决这个问题，不过虽然在Linux上创建进程的代价比较小，但仍旧是不可忽视的。另外，进程间的数据交换非常麻烦。值得庆幸的是，epoll并没有这个限制，它所支持的FD上限是操作系统的最大文件句柄数，这个数字远远大于1024。具体的值可以通过cat /proc/sys/fs/file- max查看，通常情况下这个值跟系统的内存关系比较大
* I/O效率不会随着FD数目的增加而线性下降。传统select/poll的另一个致命弱点，就是当拥有一个很大的socket集合时，由于网络延时或者链路空闲，任一时刻只有少部分的socket是“活跃”的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。**epoll不存在这个问题，它只会对“活跃”的socket进行操作。这是因为在内核实现中，epoll是根据每个fd上面的callback函数实现的**。那么，只有“活跃”的socket才会去主动调用callback函数，其他idle状态的socket则不会。在这点上，epoll实现了一个伪AIO
* 使用mmap加速内核与用户空间的消息传递。无论是select、poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存复制就显得非常重要，epoll是通过内核和用户空间mmap同一块内存来实现的
* epoll 的API更加简单。包括创建一个epoll描述符、添加监听事件、阻塞等待所监听的事件发生、关闭epoll描述符等

值得说明的是，用来克服select/poll缺点的方法不只有epoll，epoll只是一种Linux的实现方案。在freeBSD下有kqueue，kqueue在许多UNIX系统上存在，包括OS X也就是macOS。kqueue是freebsd的宠儿，它实际上是一个功能相当丰富的kernel事件队列，它不仅仅是select/poll的升级，而且可以处理signal、目录结构变化、进程等多种事件。kqueue是边缘触发的

### 物理CPU、CPU核数、逻辑CPU、超线程

https://blog.csdn.net/swordgirl2011/article/details/84444493